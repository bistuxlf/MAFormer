[ 2023-04-26 09:55 ] Model load finished: model.sttformer_sta.Model
[ 2023-04-26 09:56 ] Data load finished
[ 2023-04-26 09:56 ] Optimizer load finished: SGD
[ 2023-04-26 09:56 ] base_lr: 0.1
[ 2023-04-26 09:56 ] batch_size: 64
[ 2023-04-26 09:56 ] config: graphformer.yaml
[ 2023-04-26 09:56 ] cuda_visible_device: 0,1
[ 2023-04-26 09:56 ] device: [0, 1]
[ 2023-04-26 09:56 ] eval_interval: 5
[ 2023-04-26 09:56 ] feeder: feeders.feeder_ntu.Feeder
[ 2023-04-26 09:56 ] ignore_weights: []
[ 2023-04-26 09:56 ] lr_decay_rate: 0.1
[ 2023-04-26 09:56 ] model: model.sttformer_sta.Model
[ 2023-04-26 09:56 ] model_args: {'len_parts': 6, 'num_frames': 120, 'num_joints': 25, 'num_classes': 60, 'num_heads': 3, 'kernel_size': [3, 5], 'num_persons': 2, 'num_channels': 3, 'use_pes': True, 'config': [[64, 64, 16], [64, 64, 16], [64, 128, 32], [128, 128, 32], [128, 256, 64], [256, 256, 64], [256, 256, 64], [256, 256, 64]]}
[ 2023-04-26 09:56 ] nesterov: True
[ 2023-04-26 09:56 ] num_epoch: 120
[ 2023-04-26 09:56 ] num_worker: 10
[ 2023-04-26 09:56 ] optimizer: SGD
[ 2023-04-26 09:56 ] print_log: True
[ 2023-04-26 09:56 ] run_mode: train
[ 2023-04-26 09:56 ] save_epoch: 60
[ 2023-04-26 09:56 ] save_score: False
[ 2023-04-26 09:56 ] show_topk: [1, 5]
[ 2023-04-26 09:56 ] start_epoch: 0
[ 2023-04-26 09:56 ] step: [60, 80, 100]
[ 2023-04-26 09:56 ] test_batch_size: 64
[ 2023-04-26 09:56 ] test_feeder_args: {'data_path': 'gendata/ntu/NTU60_XView.npz', 'split': 'test', 'debug': False, 'window_size': 120, 'p_interval': [0.95], 'vel': True, 'bone': True}
[ 2023-04-26 09:56 ] train_feeder_args: {'data_path': 'gendata/ntu/NTU60_XView.npz', 'split': 'train', 'debug': False, 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': 120, 'normalization': False, 'random_rot': True, 'p_interval': [0.5, 1], 'vel': True, 'bone': True}
[ 2023-04-26 09:56 ] warm_up_epoch: 5
[ 2023-04-26 09:56 ] weight_decay: 0.0005
[ 2023-04-26 09:56 ] weights: None
[ 2023-04-26 09:56 ] work_dir: ./gf_weight_l6/ntu60/xview/bone_motion
[ 2023-04-26 09:56 ] # Parameters: 6256088
[ 2023-04-26 09:56 ] ###***************start training***************###
[ 2023-04-26 09:56 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 10:03 ] training: epoch: 1, loss: 2.6348, top1: 27.71%, lr: 0.020000
[ 2023-04-26 10:03 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 10:12 ] training: epoch: 2, loss: 1.7533, top1: 48.20%, lr: 0.040000
[ 2023-04-26 10:12 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 10:21 ] training: epoch: 3, loss: 1.4331, top1: 56.77%, lr: 0.060000
[ 2023-04-26 10:21 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 10:30 ] training: epoch: 4, loss: 1.2806, top1: 61.35%, lr: 0.080000
[ 2023-04-26 10:30 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 10:39 ] training: epoch: 5, loss: 1.1823, top1: 64.13%, lr: 0.100000
[ 2023-04-26 10:40 ] evaluating: loss: 2.3188, top1: 39.26%, best_acc: 39.26%
[ 2023-04-26 10:40 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 10:49 ] training: epoch: 6, loss: 1.0836, top1: 66.63%, lr: 0.100000
[ 2023-04-26 10:49 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 10:58 ] training: epoch: 7, loss: 1.0193, top1: 68.70%, lr: 0.100000
[ 2023-04-26 10:58 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 11:07 ] training: epoch: 8, loss: 0.9656, top1: 69.94%, lr: 0.100000
[ 2023-04-26 11:07 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 11:16 ] training: epoch: 9, loss: 0.9488, top1: 70.73%, lr: 0.100000
[ 2023-04-26 11:16 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 11:25 ] training: epoch: 10, loss: 0.9218, top1: 71.44%, lr: 0.100000
[ 2023-04-26 11:27 ] evaluating: loss: 4.4524, top1: 27.07%, best_acc: 39.26%
[ 2023-04-26 11:27 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 11:36 ] training: epoch: 11, loss: 0.8998, top1: 71.95%, lr: 0.100000
[ 2023-04-26 11:36 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 11:45 ] training: epoch: 12, loss: 0.8879, top1: 72.43%, lr: 0.100000
[ 2023-04-26 11:45 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 11:53 ] training: epoch: 13, loss: 0.8756, top1: 72.89%, lr: 0.100000
[ 2023-04-26 11:53 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 12:02 ] training: epoch: 14, loss: 0.8604, top1: 73.29%, lr: 0.100000
[ 2023-04-26 12:02 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 12:11 ] training: epoch: 15, loss: 0.8401, top1: 73.75%, lr: 0.100000
[ 2023-04-26 12:13 ] evaluating: loss: 4.7700, top1: 25.41%, best_acc: 39.26%
[ 2023-04-26 12:13 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 12:21 ] training: epoch: 16, loss: 0.8334, top1: 74.04%, lr: 0.100000
[ 2023-04-26 12:21 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 12:30 ] training: epoch: 17, loss: 0.8261, top1: 73.98%, lr: 0.100000
[ 2023-04-26 12:30 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 12:39 ] training: epoch: 18, loss: 0.8167, top1: 74.64%, lr: 0.100000
[ 2023-04-26 12:39 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 12:48 ] training: epoch: 19, loss: 0.8093, top1: 74.65%, lr: 0.100000
[ 2023-04-26 12:48 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 12:56 ] training: epoch: 20, loss: 0.7980, top1: 75.07%, lr: 0.100000
[ 2023-04-26 12:58 ] evaluating: loss: 1.9082, top1: 57.47%, best_acc: 57.47%
[ 2023-04-26 12:58 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 13:07 ] training: epoch: 21, loss: 0.7924, top1: 75.32%, lr: 0.100000
[ 2023-04-26 13:07 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 13:16 ] training: epoch: 22, loss: 0.7956, top1: 75.09%, lr: 0.100000
[ 2023-04-26 13:16 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 13:24 ] training: epoch: 23, loss: 0.7877, top1: 75.49%, lr: 0.100000
[ 2023-04-26 13:24 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 13:33 ] training: epoch: 24, loss: 0.7764, top1: 75.77%, lr: 0.100000
[ 2023-04-26 13:33 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 13:42 ] training: epoch: 25, loss: 0.7791, top1: 75.54%, lr: 0.100000
[ 2023-04-26 13:44 ] evaluating: loss: 1.8036, top1: 54.10%, best_acc: 57.47%
[ 2023-04-26 13:44 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 13:53 ] training: epoch: 26, loss: 0.7649, top1: 76.08%, lr: 0.100000
[ 2023-04-26 13:53 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 14:02 ] training: epoch: 27, loss: 0.7662, top1: 76.09%, lr: 0.100000
[ 2023-04-26 14:02 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 14:10 ] training: epoch: 28, loss: 0.7662, top1: 76.16%, lr: 0.100000
[ 2023-04-26 14:10 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 14:19 ] training: epoch: 29, loss: 0.7576, top1: 76.50%, lr: 0.100000
[ 2023-04-26 14:19 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 14:28 ] training: epoch: 30, loss: 0.7573, top1: 76.48%, lr: 0.100000
[ 2023-04-26 14:30 ] evaluating: loss: 3.8934, top1: 35.79%, best_acc: 57.47%
[ 2023-04-26 14:30 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 14:39 ] training: epoch: 31, loss: 0.7589, top1: 76.30%, lr: 0.100000
[ 2023-04-26 14:39 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 14:48 ] training: epoch: 32, loss: 0.7503, top1: 76.41%, lr: 0.100000
[ 2023-04-26 14:48 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 14:56 ] training: epoch: 33, loss: 0.7453, top1: 76.80%, lr: 0.100000
[ 2023-04-26 14:56 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 15:05 ] training: epoch: 34, loss: 0.7487, top1: 76.74%, lr: 0.100000
[ 2023-04-26 15:05 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 15:14 ] training: epoch: 35, loss: 0.7430, top1: 76.90%, lr: 0.100000
[ 2023-04-26 15:16 ] evaluating: loss: 0.6977, top1: 77.73%, best_acc: 77.73%
[ 2023-04-26 15:16 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 15:25 ] training: epoch: 36, loss: 0.7360, top1: 76.80%, lr: 0.100000
[ 2023-04-26 15:25 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 15:34 ] training: epoch: 37, loss: 0.7439, top1: 76.79%, lr: 0.100000
[ 2023-04-26 15:34 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 15:43 ] training: epoch: 38, loss: 0.7447, top1: 76.60%, lr: 0.100000
[ 2023-04-26 15:43 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 15:52 ] training: epoch: 39, loss: 0.7300, top1: 77.44%, lr: 0.100000
[ 2023-04-26 15:52 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 16:01 ] training: epoch: 40, loss: 0.7357, top1: 76.98%, lr: 0.100000
[ 2023-04-26 16:03 ] evaluating: loss: 0.9982, top1: 69.87%, best_acc: 77.73%
[ 2023-04-26 16:03 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 16:12 ] training: epoch: 41, loss: 0.7361, top1: 77.06%, lr: 0.100000
[ 2023-04-26 16:12 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 16:21 ] training: epoch: 42, loss: 0.7211, top1: 77.34%, lr: 0.100000
[ 2023-04-26 16:21 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 16:30 ] training: epoch: 43, loss: 0.7280, top1: 77.19%, lr: 0.100000
[ 2023-04-26 16:30 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 16:39 ] training: epoch: 44, loss: 0.7185, top1: 77.58%, lr: 0.100000
[ 2023-04-26 16:39 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 16:48 ] training: epoch: 45, loss: 0.7267, top1: 77.37%, lr: 0.100000
[ 2023-04-26 16:50 ] evaluating: loss: 0.6895, top1: 78.51%, best_acc: 78.51%
[ 2023-04-26 16:50 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 16:59 ] training: epoch: 46, loss: 0.7174, top1: 77.44%, lr: 0.100000
[ 2023-04-26 16:59 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 17:08 ] training: epoch: 47, loss: 0.7227, top1: 77.59%, lr: 0.100000
[ 2023-04-26 17:08 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 17:16 ] training: epoch: 48, loss: 0.7121, top1: 77.62%, lr: 0.100000
[ 2023-04-26 17:16 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 17:25 ] training: epoch: 49, loss: 0.7123, top1: 77.97%, lr: 0.100000
[ 2023-04-26 17:25 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 17:34 ] training: epoch: 50, loss: 0.7078, top1: 77.90%, lr: 0.100000
[ 2023-04-26 17:36 ] evaluating: loss: 2.5394, top1: 42.93%, best_acc: 78.51%
[ 2023-04-26 17:36 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 17:44 ] training: epoch: 51, loss: 0.7046, top1: 78.26%, lr: 0.100000
[ 2023-04-26 17:44 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 17:53 ] training: epoch: 52, loss: 0.7088, top1: 77.75%, lr: 0.100000
[ 2023-04-26 17:53 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 18:02 ] training: epoch: 53, loss: 0.7027, top1: 77.91%, lr: 0.100000
[ 2023-04-26 18:02 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 18:11 ] training: epoch: 54, loss: 0.7067, top1: 77.80%, lr: 0.100000
[ 2023-04-26 18:11 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 18:20 ] training: epoch: 55, loss: 0.7021, top1: 78.13%, lr: 0.100000
[ 2023-04-26 18:21 ] evaluating: loss: 1.3140, top1: 61.76%, best_acc: 78.51%
[ 2023-04-26 18:21 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 18:30 ] training: epoch: 56, loss: 0.6996, top1: 78.17%, lr: 0.100000
[ 2023-04-26 18:30 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 18:39 ] training: epoch: 57, loss: 0.7021, top1: 78.21%, lr: 0.100000
[ 2023-04-26 18:39 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 18:48 ] training: epoch: 58, loss: 0.7077, top1: 78.07%, lr: 0.100000
[ 2023-04-26 18:48 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 18:56 ] training: epoch: 59, loss: 0.6983, top1: 78.12%, lr: 0.100000
[ 2023-04-26 18:56 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 19:05 ] training: epoch: 60, loss: 0.6932, top1: 78.45%, lr: 0.100000
[ 2023-04-26 19:07 ] evaluating: loss: 1.6501, top1: 55.96%, best_acc: 78.51%
[ 2023-04-26 19:07 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 19:15 ] training: epoch: 61, loss: 0.4437, top1: 86.19%, lr: 0.010000
[ 2023-04-26 19:17 ] evaluating: loss: 0.3151, top1: 89.49%, best_acc: 89.49%
[ 2023-04-26 19:17 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 19:26 ] training: epoch: 62, loss: 0.3666, top1: 88.37%, lr: 0.010000
[ 2023-04-26 19:28 ] evaluating: loss: 0.3070, top1: 89.60%, best_acc: 89.60%
[ 2023-04-26 19:28 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 19:36 ] training: epoch: 63, loss: 0.3353, top1: 89.41%, lr: 0.010000
[ 2023-04-26 19:38 ] evaluating: loss: 0.2990, top1: 89.90%, best_acc: 89.90%
[ 2023-04-26 19:38 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 19:47 ] training: epoch: 64, loss: 0.3074, top1: 90.34%, lr: 0.010000
[ 2023-04-26 19:48 ] evaluating: loss: 0.2909, top1: 90.26%, best_acc: 90.26%
[ 2023-04-26 19:48 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 19:57 ] training: epoch: 65, loss: 0.2925, top1: 90.83%, lr: 0.010000
[ 2023-04-26 19:59 ] evaluating: loss: 0.2907, top1: 90.14%, best_acc: 90.26%
[ 2023-04-26 19:59 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 20:07 ] training: epoch: 66, loss: 0.2779, top1: 91.25%, lr: 0.010000
[ 2023-04-26 20:09 ] evaluating: loss: 0.2822, top1: 90.53%, best_acc: 90.53%
[ 2023-04-26 20:09 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 20:18 ] training: epoch: 67, loss: 0.2623, top1: 91.70%, lr: 0.010000
[ 2023-04-26 20:19 ] evaluating: loss: 0.2946, top1: 90.32%, best_acc: 90.53%
[ 2023-04-26 20:19 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 20:28 ] training: epoch: 68, loss: 0.2507, top1: 92.10%, lr: 0.010000
[ 2023-04-26 20:30 ] evaluating: loss: 0.2918, top1: 90.34%, best_acc: 90.53%
[ 2023-04-26 20:30 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 20:39 ] training: epoch: 69, loss: 0.2365, top1: 92.60%, lr: 0.010000
[ 2023-04-26 20:40 ] evaluating: loss: 0.3040, top1: 90.08%, best_acc: 90.53%
[ 2023-04-26 20:40 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 20:49 ] training: epoch: 70, loss: 0.2333, top1: 92.77%, lr: 0.010000
[ 2023-04-26 20:51 ] evaluating: loss: 0.3001, top1: 90.11%, best_acc: 90.53%
[ 2023-04-26 20:51 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 20:59 ] training: epoch: 71, loss: 0.2260, top1: 92.84%, lr: 0.010000
[ 2023-04-26 21:01 ] evaluating: loss: 0.2880, top1: 90.44%, best_acc: 90.53%
[ 2023-04-26 21:01 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 21:10 ] training: epoch: 72, loss: 0.2173, top1: 93.23%, lr: 0.010000
[ 2023-04-26 21:11 ] evaluating: loss: 0.3178, top1: 89.71%, best_acc: 90.53%
[ 2023-04-26 21:11 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 21:20 ] training: epoch: 73, loss: 0.2128, top1: 93.37%, lr: 0.010000
[ 2023-04-26 21:22 ] evaluating: loss: 0.3001, top1: 90.54%, best_acc: 90.54%
[ 2023-04-26 21:22 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 21:30 ] training: epoch: 74, loss: 0.2116, top1: 93.46%, lr: 0.010000
[ 2023-04-26 21:32 ] evaluating: loss: 0.3243, top1: 89.93%, best_acc: 90.54%
[ 2023-04-26 21:32 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 21:41 ] training: epoch: 75, loss: 0.2055, top1: 93.57%, lr: 0.010000
[ 2023-04-26 21:42 ] evaluating: loss: 0.3253, top1: 89.58%, best_acc: 90.54%
[ 2023-04-26 21:42 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 21:51 ] training: epoch: 76, loss: 0.2021, top1: 93.60%, lr: 0.010000
[ 2023-04-26 21:53 ] evaluating: loss: 0.3141, top1: 89.96%, best_acc: 90.54%
[ 2023-04-26 21:53 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 22:02 ] training: epoch: 77, loss: 0.1976, top1: 93.90%, lr: 0.010000
[ 2023-04-26 22:03 ] evaluating: loss: 0.3239, top1: 89.92%, best_acc: 90.54%
[ 2023-04-26 22:03 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 22:12 ] training: epoch: 78, loss: 0.1948, top1: 93.99%, lr: 0.010000
[ 2023-04-26 22:14 ] evaluating: loss: 0.3421, top1: 89.49%, best_acc: 90.54%
[ 2023-04-26 22:14 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 22:22 ] training: epoch: 79, loss: 0.1967, top1: 93.88%, lr: 0.010000
[ 2023-04-26 22:24 ] evaluating: loss: 0.3274, top1: 89.64%, best_acc: 90.54%
[ 2023-04-26 22:24 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 22:33 ] training: epoch: 80, loss: 0.1905, top1: 94.09%, lr: 0.010000
[ 2023-04-26 22:34 ] evaluating: loss: 0.3274, top1: 89.85%, best_acc: 90.54%
[ 2023-04-26 22:34 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 22:43 ] training: epoch: 81, loss: 0.1286, top1: 96.32%, lr: 0.001000
[ 2023-04-26 22:45 ] evaluating: loss: 0.2691, top1: 91.23%, best_acc: 91.23%
[ 2023-04-26 22:45 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 22:54 ] training: epoch: 82, loss: 0.1019, top1: 97.14%, lr: 0.001000
[ 2023-04-26 22:55 ] evaluating: loss: 0.2693, top1: 91.36%, best_acc: 91.36%
[ 2023-04-26 22:55 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 23:04 ] training: epoch: 83, loss: 0.0924, top1: 97.52%, lr: 0.001000
[ 2023-04-26 23:06 ] evaluating: loss: 0.2714, top1: 91.50%, best_acc: 91.50%
[ 2023-04-26 23:06 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 23:14 ] training: epoch: 84, loss: 0.0828, top1: 97.84%, lr: 0.001000
[ 2023-04-26 23:16 ] evaluating: loss: 0.2746, top1: 91.42%, best_acc: 91.50%
[ 2023-04-26 23:16 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 23:25 ] training: epoch: 85, loss: 0.0789, top1: 97.98%, lr: 0.001000
[ 2023-04-26 23:26 ] evaluating: loss: 0.2750, top1: 91.45%, best_acc: 91.50%
[ 2023-04-26 23:26 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 23:35 ] training: epoch: 86, loss: 0.0742, top1: 98.04%, lr: 0.001000
[ 2023-04-26 23:37 ] evaluating: loss: 0.2780, top1: 91.43%, best_acc: 91.50%
[ 2023-04-26 23:37 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 23:45 ] training: epoch: 87, loss: 0.0729, top1: 98.05%, lr: 0.001000
[ 2023-04-26 23:47 ] evaluating: loss: 0.2814, top1: 91.40%, best_acc: 91.50%
[ 2023-04-26 23:47 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-26 23:55 ] training: epoch: 88, loss: 0.0700, top1: 98.23%, lr: 0.001000
[ 2023-04-26 23:57 ] evaluating: loss: 0.2782, top1: 91.36%, best_acc: 91.50%
[ 2023-04-26 23:57 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 00:06 ] training: epoch: 89, loss: 0.0656, top1: 98.34%, lr: 0.001000
[ 2023-04-27 00:08 ] evaluating: loss: 0.2785, top1: 91.50%, best_acc: 91.50%
[ 2023-04-27 00:08 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 00:17 ] training: epoch: 90, loss: 0.0626, top1: 98.43%, lr: 0.001000
[ 2023-04-27 00:19 ] evaluating: loss: 0.2787, top1: 91.41%, best_acc: 91.50%
[ 2023-04-27 00:19 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 00:28 ] training: epoch: 91, loss: 0.0643, top1: 98.32%, lr: 0.001000
[ 2023-04-27 00:29 ] evaluating: loss: 0.2859, top1: 91.46%, best_acc: 91.50%
[ 2023-04-27 00:29 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 00:38 ] training: epoch: 92, loss: 0.0580, top1: 98.58%, lr: 0.001000
[ 2023-04-27 00:40 ] evaluating: loss: 0.2846, top1: 91.41%, best_acc: 91.50%
[ 2023-04-27 00:40 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 00:49 ] training: epoch: 93, loss: 0.0572, top1: 98.58%, lr: 0.001000
[ 2023-04-27 00:51 ] evaluating: loss: 0.2847, top1: 91.53%, best_acc: 91.53%
[ 2023-04-27 00:51 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 01:00 ] training: epoch: 94, loss: 0.0559, top1: 98.59%, lr: 0.001000
[ 2023-04-27 01:02 ] evaluating: loss: 0.2959, top1: 91.39%, best_acc: 91.53%
[ 2023-04-27 01:02 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 01:11 ] training: epoch: 95, loss: 0.0550, top1: 98.62%, lr: 0.001000
[ 2023-04-27 01:13 ] evaluating: loss: 0.2917, top1: 91.31%, best_acc: 91.53%
[ 2023-04-27 01:13 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 01:22 ] training: epoch: 96, loss: 0.0524, top1: 98.68%, lr: 0.001000
[ 2023-04-27 01:24 ] evaluating: loss: 0.2854, top1: 91.42%, best_acc: 91.53%
[ 2023-04-27 01:24 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 01:33 ] training: epoch: 97, loss: 0.0497, top1: 98.81%, lr: 0.001000
[ 2023-04-27 01:34 ] evaluating: loss: 0.2897, top1: 91.44%, best_acc: 91.53%
[ 2023-04-27 01:34 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 01:44 ] training: epoch: 98, loss: 0.0492, top1: 98.85%, lr: 0.001000
[ 2023-04-27 01:45 ] evaluating: loss: 0.2910, top1: 91.53%, best_acc: 91.53%
[ 2023-04-27 01:45 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 01:55 ] training: epoch: 99, loss: 0.0472, top1: 98.93%, lr: 0.001000
[ 2023-04-27 01:56 ] evaluating: loss: 0.2898, top1: 91.53%, best_acc: 91.53%
[ 2023-04-27 01:56 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 02:05 ] training: epoch: 100, loss: 0.0456, top1: 99.04%, lr: 0.001000
[ 2023-04-27 02:07 ] evaluating: loss: 0.2937, top1: 91.51%, best_acc: 91.53%
[ 2023-04-27 02:07 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 02:16 ] training: epoch: 101, loss: 0.0414, top1: 99.11%, lr: 0.000100
[ 2023-04-27 02:18 ] evaluating: loss: 0.2957, top1: 91.52%, best_acc: 91.53%
[ 2023-04-27 02:18 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 02:27 ] training: epoch: 102, loss: 0.0425, top1: 99.05%, lr: 0.000100
[ 2023-04-27 02:29 ] evaluating: loss: 0.2940, top1: 91.42%, best_acc: 91.53%
[ 2023-04-27 02:29 ] adjust learning rate, using warm up, epoch: 5
[ 2023-04-27 02:38 ] training: epoch: 103, loss: 0.0413, top1: 99.10%, lr: 0.000100
[ 2023-04-27 02:40 ] evaluating: loss: 0.2970, top1: 91.45%, best_acc: 91.53%
[ 2023-04-27 02:40 ] adjust learning rate, using warm up, epoch: 5
